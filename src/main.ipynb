{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989e329a",
   "metadata": {},
   "source": [
    "创建Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor, LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"HousePrices\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac40ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20 rows, Test: 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd().parent /'files'\n",
    "\n",
    "# 测试数据\n",
    "train_path = str(DATA_DIR/\"train_simple.csv\")\n",
    "test_path  = str(DATA_DIR/\"test_simple.csv\")\n",
    "\n",
    "train_spark = spark.read.option(\"header\", \"true\").csv(train_path, inferSchema=True)\n",
    "test_spark  = spark.read.option(\"header\", \"true\").csv(test_path,  inferSchema=True)\n",
    "\n",
    "print(f\"Train: {train_spark.count()} rows, Test: {test_spark.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331df52",
   "metadata": {},
   "source": [
    "缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3384253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 类别型 NA → \"None\" ----------\n",
    "na_to_none_cols = [\n",
    "    'Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "    'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n",
    "    'PoolQC','Fence','MiscFeature'\n",
    "]\n",
    "\n",
    "for col in na_to_none_cols:\n",
    "    train_spark = train_spark.withColumn(col, F.when(F.col(col).isNull(), \"None\").otherwise(F.col(col)))\n",
    "    test_spark  = test_spark.withColumn(col,  F.when(F.col(col).isNull(), \"None\").otherwise(F.col(col)))\n",
    "\n",
    "# ---------- LotFrontage：按 Neighborhood 中位数填充 ----------\n",
    "lotfront_median = (\n",
    "    train_spark.groupBy(\"Neighborhood\")\n",
    "    .agg(F.percentile_approx(\"LotFrontage\", 0.5).alias(\"median_lot\"))\n",
    ")\n",
    "\n",
    "train_spark = train_spark.join(lotfront_median, on=\"Neighborhood\", how=\"left\")\n",
    "test_spark  = test_spark.join(lotfront_median,  on=\"Neighborhood\", how=\"left\")\n",
    "\n",
    "train_spark = train_spark.withColumn(\n",
    "    \"LotFrontage\",\n",
    "    F.when(F.col(\"LotFrontage\").isNull(), F.col(\"median_lot\")).otherwise(F.col(\"LotFrontage\"))\n",
    ")\n",
    "test_spark = test_spark.withColumn(\n",
    "    \"LotFrontage\",\n",
    "    F.when(F.col(\"LotFrontage\").isNull(), F.col(\"median_lot\")).otherwise(F.col(\"LotFrontage\"))\n",
    ")\n",
    "\n",
    "train_spark = train_spark.drop(\"median_lot\")\n",
    "test_spark  = test_spark.drop(\"median_lot\")\n",
    "\n",
    "# ---------- 其余数值型缺失 → 中位数 ----------\n",
    "num_cols = [f.name for f in train_spark.schema.fields if isinstance(f.dataType, (IntegerType, DoubleType, LongType))]\n",
    "num_cols = [c for c in num_cols if c not in ['Id', 'SalePrice']]\n",
    "\n",
    "for col in num_cols:\n",
    "    median_val = train_spark.approxQuantile(col, [0.5], 0.01)[0]\n",
    "    train_spark = train_spark.withColumn(col, F.when(F.col(col).isNull(), median_val).otherwise(F.col(col)))\n",
    "    test_spark  = test_spark.withColumn(col,  F.when(F.col(col).isNull(), median_val).otherwise(F.col(col)))\n",
    "\n",
    "# GarageYrBlt → 无车库设为 0\n",
    "train_spark = train_spark.withColumn(\"GarageYrBlt\", F.when(F.col(\"GarageYrBlt\").isNull(), 0).otherwise(F.col(\"GarageYrBlt\")))\n",
    "test_spark  = test_spark.withColumn(\"GarageYrBlt\",  F.when(F.col(\"GarageYrBlt\").isNull(), 0).otherwise(F.col(\"GarageYrBlt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b794b",
   "metadata": {},
   "source": [
    "特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36d5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, year, lit\n",
    "\n",
    "current_year = 2025\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df = df.withColumn(\"HouseAge\",   lit(current_year) - col(\"YearBuilt\"))\n",
    "    df = df.withColumn(\"RemodAge\",   lit(current_year) - col(\"YearRemodAdd\"))\n",
    "    df = df.withColumn(\"GarageAge\",  lit(current_year) - col(\"GarageYrBlt\"))\n",
    "\n",
    "    df = df.withColumn(\"TotalSF\",    col(\"TotalBsmtSF\") + col(\"1stFlrSF\") + col(\"2ndFlrSF\"))\n",
    "    df = df.withColumn(\"TotalBath\",  col(\"FullBath\") + 0.5*col(\"HalfBath\") +\n",
    "                                    col(\"BsmtFullBath\") + 0.5*col(\"BsmtHalfBath\"))\n",
    "\n",
    "    df = df.withColumn(\"HasPool\",     (col(\"PoolArea\") > 0).cast(\"int\"))\n",
    "    df = df.withColumn(\"Has2ndFloor\", (col(\"2ndFlrSF\") > 0).cast(\"int\"))\n",
    "    df = df.withColumn(\"HasGarage\",   (col(\"GarageArea\") > 0).cast(\"int\"))\n",
    "    df = df.withColumn(\"HasBsmt\",     (col(\"TotalBsmtSF\") > 0).cast(\"int\"))\n",
    "\n",
    "    df = df.withColumn(\"OverallGrade\", col(\"OverallQual\") * col(\"OverallCond\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "train_spark = feature_engineering(train_spark)\n",
    "test_spark  = feature_engineering(test_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c926256",
   "metadata": {},
   "source": [
    "目标变量处理+变量拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05248ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat: 44, Num: 41, Bool: 4\n"
     ]
    }
   ],
   "source": [
    "# 对 SalePrice 取 log1p\n",
    "train_spark = train_spark.withColumn(\"log_SalePrice\", F.log1p(col(\"SalePrice\")))\n",
    "\n",
    "# 特征列\n",
    "cat_cols = [f.name for f in train_spark.schema.fields if isinstance(f.dataType, StringType)]\n",
    "num_cols = [f.name for f in train_spark.schema.fields \n",
    "            if isinstance(f.dataType, (IntegerType, DoubleType, LongType))\n",
    "            and f.name not in ['Id', 'SalePrice', 'log_SalePrice']]\n",
    "\n",
    "bool_cols = ['HasPool','Has2ndFloor','HasGarage','HasBsmt']\n",
    "num_cols = [c for c in num_cols if c not in bool_cols]\n",
    "\n",
    "print(f\"Cat: {len(cat_cols)}, Num: {len(num_cols)}, Bool: {len(bool_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a55142",
   "metadata": {},
   "source": [
    "PySpark ML Pipeline（StringIndexer + OneHot + StandardScaler）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9537bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. StringIndexer\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\")\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# 2. OneHotEncoder\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_ohe\", handleInvalid=\"keep\")\n",
    "    for col in cat_cols\n",
    "]\n",
    "\n",
    "# 3. VectorAssembler\n",
    "ohe_cols = [col+\"_ohe\" for col in cat_cols]\n",
    "assembler_inputs = ohe_cols + num_cols + bool_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# 4. 标准化\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# 5. 模型（GBT 回归，MLlib 自带）\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"log_SalePrice\",\n",
    "    maxDepth=6,\n",
    "    maxIter=200,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 完整 Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436be94",
   "metadata": {},
   "source": [
    "交叉验证 & 超参数搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"log_SalePrice\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [5, 6])\n",
    "             .addGrid(gbt.maxIter, [150, 250])\n",
    "             .addGrid(gbt.subsamplingRate, [0.7, 0.85])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42,\n",
    "    parallelism=4\n",
    ")\n",
    "\n",
    "# 训练（完整数据约 5-8 分钟）\n",
    "cv_model = cv.fit(train_spark)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "print(\"Best CV RMSE (log):\", evaluator.evaluate(best_model.transform(train_spark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43572980",
   "metadata": {},
   "source": [
    "预测test，生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad550bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "pred_spark = best_model.transform(test_spark)\n",
    "\n",
    "# 逆变换\n",
    "pred_spark = pred_spark.withColumn(\"SalePrice\", F.expm1(col(\"prediction\")))\n",
    "\n",
    "# 转为 Pandas 并保存\n",
    "submission = pred_spark.select(\"Id\", \"SalePrice\").toPandas()\n",
    "submission['Id'] = submission['Id'].astype(int)\n",
    "\n",
    "result_path = str(DATA_DIR / \"submission_spark.csv\")\n",
    "submission.to_csv(result_path, index=False)\n",
    "\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
