{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HousePricePrediction\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f661c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / 'files'\n",
    "train_path = str(DATA_DIR / \"train.csv\")\n",
    "test_path  = str(DATA_DIR/\"test.csv\")\n",
    "\n",
    "train_df = (spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")   # 关键：自动推断数值类型\n",
    "            .option(\"nullValue\", \"NA\")       # 可选：明确 NA 为 null\n",
    "            .csv(train_path))\n",
    "\n",
    "test_df = (spark.read\n",
    "           .option(\"header\", \"true\")\n",
    "           .option(\"inferSchema\", \"true\")\n",
    "           .option(\"nullValue\", \"NA\")\n",
    "           .csv(test_path))\n",
    "\n",
    "print(\"=== Train Schema ===\")\n",
    "train_df.printSchema()\n",
    "print(f\"Train rows: {train_df.count()}, cols: {len(train_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== Test Schema ===\")\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 区分列类型（基于实际 schema）===\n",
    "\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "\n",
    "num_cols = [field.name for field in train_df.schema.fields\n",
    "            if isinstance(field.dataType, (IntegerType, DoubleType))\n",
    "            and field.name != \"SalePrice\"]\n",
    "\n",
    "cat_cols = [field.name for field in train_df.schema.fields\n",
    "            if isinstance(field.dataType, StringType)]\n",
    "\n",
    "print(f\"数值列 ({len(num_cols)}): {num_cols[:10]}...\")\n",
    "print(f\"类别列 ({len(cat_cols)}): {cat_cols[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3678058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# === 数值列：中位数填补 ===\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=num_cols, strategy=\"median\")\n",
    "\n",
    "# 分别 fit，避免 train/test 统计泄露\n",
    "train_df = imputer.fit(train_df).transform(train_df)\n",
    "test_df  = imputer.fit(test_df).transform(test_df)\n",
    "\n",
    "# === 类别列：缺失值填 \"Missing\" ===\n",
    "for c in cat_cols:\n",
    "    train_df = train_df.fillna({c: \"Missing\"})\n",
    "    test_df  = test_df.fillna({c: \"Missing\"})\n",
    "\n",
    "# 验证无缺失\n",
    "print(\"数值列缺失统计:\")\n",
    "train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in num_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707db875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 类别编码：StringIndexer + OneHotEncoder ===\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c + \"_idx\", outputCol=c + \"_vec\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "# 数值特征标准化\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "assembler_num = VectorAssembler(inputCols=num_cols, outputCol=\"num_features\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num\", withStd=True, withMean=True)\n",
    "\n",
    "# 最终特征向量\n",
    "feature_cols = [c + \"_vec\" for c in cat_cols] + [\"scaled_num\"]\n",
    "final_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# 目标：对 SalePrice 取 log\n",
    "import pyspark.sql.functions as F\n",
    "train_df = train_df.withColumn(\"logSalePrice\", F.log1p(F.col(\"SalePrice\")))\n",
    "target_col = \"logSalePrice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 构建完整 Pipeline ===\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    numTrees=300,\n",
    "    maxDepth=12,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline(stages=indexers + encoders + [assembler_num, scaler, final_assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3353630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model_rf = pipeline_rf.fit(train_set)\n",
    "pred_rf = model_rf.transform(val_set)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, metricName=\"rmse\")\n",
    "rmse_rf = evaluator.evaluate(pred_rf)\n",
    "r2_rf = RegressionEvaluator(labelCol=target_col, metricName=\"r2\").evaluate(pred_rf)\n",
    "print(f\"RF RMSE (log): {rmse_rf:.4f}, R²: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# === 预测 test 并生成 submission ===\n",
    "test_pred = model_rf.transform(test_df)\n",
    "test_pred = test_pred.withColumn(\"SalePrice_pred\", F.exp(F.col(\"prediction\")) - 1)\n",
    "\n",
    "# 构造时间戳子目录：格式 2025-11-11-14-30-00\n",
    "time_str   = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "RESULT_DIR = Path.cwd().parent / 'results' / time_str\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 先写到临时目录（Spark 默认行为）\n",
    "temp_out   = RESULT_DIR / \"temp_csv\"\n",
    "result = test_pred.select(\"Id\", \"SalePrice_pred\").withColumnRenamed(\"SalePrice_pred\", \"SalePrice\")\n",
    "result.coalesce(1).write.csv(str(temp_out), header=True, mode=\"overwrite\")\n",
    "\n",
    "# 找到真正的 csv 文件并重命名为 submission.csv\n",
    "csv_file = list(temp_out.glob(\"part-*-*.csv\"))[0]\n",
    "final_file = RESULT_DIR / \"submission.csv\"\n",
    "shutil.move(str(csv_file), str(final_file))\n",
    "\n",
    "# 清理 Spark 生成的冗余文件（_SUCCESS、.crc 等）\n",
    "shutil.rmtree(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca6004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c280521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
